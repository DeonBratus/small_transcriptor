{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq_mWK0BdDj1",
        "outputId": "9010a27a-c597-4b6c-8b92-5e345df92c1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/debral/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/home/debral/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyannote.audio import Pipeline\n",
        "import whisper\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1ac4a3f460914480b67d949b10cd4ec3",
            "ba6fe69e94234c9f8cb2dfb915dc81ca",
            "99e8534b9e6f4e5ea0fb84e0c7bc7978",
            "cf63ebb00db345fb8b08eed47100fd78",
            "a2446ebe2dd74a588a8c1f0705bfd766"
          ]
        },
        "id": "RH8YN7IldLlp",
        "outputId": "48e4d674-7c89-447a-c10b-899d12e9d5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загрузка моделей...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============= Настройки =============\n",
        "file_name = \"32140-312ds213-91094-2134.mp3\"   # путь к аудио\n",
        "num_speakers = 4                             # количество спикеров\n",
        "model_name = \"base\"                          # whisper: tiny / base / small / medium / large\n",
        "\n",
        "# Hugging Face токен\n",
        "hf_auth_token = \"\"\n",
        "\n",
        "# ============= Загрузка моделей =============\n",
        "print(\"Загрузка моделей...\")\n",
        "model = whisper.load_model(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmMQ7DY0dLq8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Запуск диаризации...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/debral/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
            "  std = sequences.std(dim=-1, correction=1)\n",
            "/home/debral/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
            "  warnings.warn(\n",
            "/home/debral/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
            "  warnings.warn(\n",
            "/home/debral/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ============= Диаризация =============\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mЗапуск диаризации...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m diarization = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_speakers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_speakers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Собираем сегменты по спикерам\u001b[39;00m\n\u001b[32m     15\u001b[39m speaker_segments = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/pyannote/audio/core/pipeline.py:325\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, file, **kwargs)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpreprocessors\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    323\u001b[39m     file = ProtocolFile(file, lazy=\u001b[38;5;28mself\u001b[39m.preprocessors)\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/pyannote/audio/pipelines/speaker_diarization.py:514\u001b[39m, in \u001b[36mSpeakerDiarization.apply\u001b[39m\u001b[34m(self, file, num_speakers, min_speakers, max_speakers, return_embeddings, hook)\u001b[39m\n\u001b[32m    512\u001b[39m     embeddings = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbinarized_segmentations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_exclude_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m     hook(\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m, embeddings)\n\u001b[32m    521\u001b[39m     \u001b[38;5;66;03m#   shape: (num_chunks, local_num_speakers, dimension)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/pyannote/audio/pipelines/speaker_diarization.py:349\u001b[39m, in \u001b[36mSpeakerDiarization.get_embeddings\u001b[39m\u001b[34m(self, file, binary_segmentations, exclude_overlap, hook)\u001b[39m\n\u001b[32m    346\u001b[39m mask_batch = torch.vstack(masks)\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# (batch_size, num_frames) torch.Tensor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m embedding_batch: np.ndarray = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwaveform_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_batch\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# (batch_size, dimension) np.ndarray\u001b[39;00m\n\u001b[32m    354\u001b[39m embedding_batches.append(embedding_batch)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/pyannote/audio/pipelines/speaker_verification.py:709\u001b[39m, in \u001b[36mPyannoteAudioPretrainedSpeakerEmbedding.__call__\u001b[39m\u001b[34m(self, waveforms, masks)\u001b[39m\n\u001b[32m    705\u001b[39m             warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    706\u001b[39m             embeddings = \u001b[38;5;28mself\u001b[39m.model_(\n\u001b[32m    707\u001b[39m                 waveforms.to(\u001b[38;5;28mself\u001b[39m.device), weights=masks.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    708\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy()\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization-3.1\",\n",
        "    use_auth_token=hf_auth_token\n",
        ")\n",
        "\n",
        "pipeline = pipeline.to(device)\n",
        "\n",
        "# ============= Диаризация =============\n",
        "print(\"Запуск диаризации...\")\n",
        "diarization = pipeline(file_name, num_speakers=num_speakers)\n",
        "\n",
        "# Собираем сегменты по спикерам\n",
        "speaker_segments = []\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    speaker_segments.append({\n",
        "        'speaker': speaker,\n",
        "        'start': turn.start,\n",
        "        'end': turn.end\n",
        "    })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EiI3dUFqdLtB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Запуск транскрибации...\n",
            "[00:00.000 --> 00:22.100]  Вот в hazdAKE армии.\n",
            "[00:28.200 --> 00:29.980]  Солф, Перзкам сums, Короли, капусты, pursue нужно завтудиться в нашим\n",
            "[00:22.260 --> 00:24.340] usterах.\n",
            "[00:29.980 --> 00:30.740]  Назад начал.\n",
            "[00:31.060 --> 00:37.260]  Сейчас до этого занимаюсь проектом Кассаема классификации\n",
            "[00:38.040 --> 00:41.820]  заявывать для GKH и в дальнейшем ответы на вопросы.\n",
            "[00:42.580 --> 00:46.020]  И сейчас пришли на более перспективные направления\n",
            "[00:46.020 --> 00:48.780]  это краски, что работает EI.\n",
            "[00:50.640 --> 00:58.440]  Это же жюри для конференции, типа КМУ, ППС, где по Тейсу,\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============= Транскрибация =============\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mЗапуск транскрибации...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mru\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mword_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/whisper/transcribe.py:295\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    293\u001b[39m     decode_options[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = all_tokens[prompt_reset_since:]\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m result: DecodingResult = \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m tokens = torch.tensor(result.tokens)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/whisper/transcribe.py:201\u001b[39m, in \u001b[36mtranscribe.<locals>.decode_with_fallback\u001b[39m\u001b[34m(segment)\u001b[39m\n\u001b[32m    198\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    200\u001b[39m options = DecodingOptions(**kwargs, temperature=t)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m decode_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m needs_fallback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    205\u001b[39m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m decode_result.compression_ratio > compression_ratio_threshold\n\u001b[32m    207\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/whisper/decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/whisper/decoding.py:737\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    734\u001b[39m tokens = tokens.repeat_interleave(\u001b[38;5;28mself\u001b[39m.n_group, dim=\u001b[32m0\u001b[39m).to(audio_features.device)\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m tokens, sum_logprobs, no_speech_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[32m    740\u001b[39m audio_features = audio_features[:: \u001b[38;5;28mself\u001b[39m.n_group]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DevSpace/small_transcriptor/.venv/lib/python3.13/site-packages/whisper/decoding.py:700\u001b[39m, in \u001b[36mDecodingTask._main_loop\u001b[39m\u001b[34m(self, audio_features, tokens)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# apply the logit filters, e.g. for suppressing or applying penalty to\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m logit_filter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logit_filters:\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     logit_filter.apply(logits, tokens)\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# expand the tokens tensor with the selected next tokens\u001b[39;00m\n\u001b[32m    703\u001b[39m tokens, completed = \u001b[38;5;28mself\u001b[39m.decoder.update(tokens, logits, sum_logprobs)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# ============= Транскрибация =============\n",
        "print(\"Запуск транскрибации...\")\n",
        "result = model.transcribe(\n",
        "    file_name,\n",
        "    language=\"ru\",\n",
        "    verbose=True,\n",
        "    word_timestamps=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdshCHtIdLv3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============= Сопоставление спикеров =============\n",
        "print(\"Сопоставление спикеров...\")\n",
        "for segment in result['segments']:\n",
        "    segment_start = segment['start']\n",
        "    segment_end = segment['end']\n",
        "\n",
        "    possible_speakers = []\n",
        "    for speaker_seg in speaker_segments:\n",
        "        overlap_start = max(speaker_seg['start'], segment_start)\n",
        "        overlap_end = min(speaker_seg['end'], segment_end)\n",
        "        if overlap_start < overlap_end:\n",
        "            overlap_duration = overlap_end - overlap_start\n",
        "            possible_speakers.append({\n",
        "                'speaker': speaker_seg['speaker'],\n",
        "                'overlap_duration': overlap_duration\n",
        "            })\n",
        "\n",
        "    speaker_label = \"Спикер неизвестен\"\n",
        "    if possible_speakers:\n",
        "        main_speaker = max(possible_speakers, key=lambda x: x['overlap_duration'])\n",
        "        speaker_label = main_speaker['speaker']\n",
        "        if len(possible_speakers) > 1:\n",
        "            speaker_label += \" (основной)\"\n",
        "\n",
        "    print(f\"[{speaker_label}] [{segment_start:.1f}-{segment_end:.1f}с]: {segment['text']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
