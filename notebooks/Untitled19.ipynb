{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzkGw5istG0M",
        "outputId": "52980cd6-2f16-41f5-8192-7211675dc04d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vosk in /usr/local/lib/python3.12/dist-packages (0.3.45)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk) (1.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vosk) (4.67.1)\n",
            "Requirement already satisfied: srt in /usr/local/lib/python3.12/dist-packages (from vosk) (3.5.3)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from vosk) (15.0.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk) (2.22)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=0dd39d096e14899f34e5f2f65a1f6c31813a534bb6e1104ed8cd11c10ec650b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n"
          ]
        }
      ],
      "source": [
        "!pip install vosk pydub openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tAI_eJD0tESb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from vosk import Model, KaldiRecognizer\n",
        "import wave\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "from collections import defaultdict\n",
        "import tempfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaQGaKLirUAu",
        "outputId": "181cfe96-978b-42d7-ede4-5b53ed5da507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Начинаем обработку MP3 файла...\n",
            "Конвертируем 32140-312ds213-91094-2134.mp3 в WAV...\n",
            "Конвертация завершена\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
            "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
            "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
            "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-ru-0.42/ivector/final.ie\n",
            "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
            "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from vosk-model-ru-0.42/graph/HCLG.fst\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:297) Loading words from vosk-model-ru-0.42/graph/words.txt\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo vosk-model-ru-0.42/graph/phones/word_boundary.int\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:315) Loading subtract G.fst model from vosk-model-ru-0.42/rescore/G.fst\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:317) Loading CARPA model from vosk-model-ru-0.42/rescore/G.carpa\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:323) Loading RNNLM model from vosk-model-ru-0.42/rnnlm/final.raw\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def convert_mp3_to_wav(mp3_path, sample_rate=16000):\n",
        "    \"\"\"\n",
        "    Конвертирует MP3 в WAV формат для Vosk\n",
        "    \"\"\"\n",
        "    print(f\"Конвертируем {mp3_path} в WAV...\")\n",
        "\n",
        "    temp_wav = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
        "    temp_wav.close()\n",
        "\n",
        "    try:\n",
        "        audio = AudioSegment.from_mp3(mp3_path)\n",
        "\n",
        "        audio = audio.set_channels(1)          # mono\n",
        "        audio = audio.set_frame_rate(sample_rate)  # 16kHz\n",
        "        audio = audio.set_sample_width(2)      # 16-bit PCM\n",
        "\n",
        "        audio.export(temp_wav.name, format=\"wav\")\n",
        "        print(\"Конвертация завершена\")\n",
        "        return temp_wav.name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка конвертации: {e}\")\n",
        "        os.unlink(temp_wav.name)\n",
        "        return None\n",
        "\n",
        "def transcribe_with_vosk(audio_path, model_path=\"vosk-model-ru-0.42\"):\n",
        "    \"\"\"\n",
        "    Транскрибация с использованием Vosk\n",
        "    \"\"\"\n",
        "\n",
        "    # Проверяем и скачиваем модель если нужно\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Модель не найдена. Скачиваем...\")\n",
        "        import urllib.request\n",
        "        import zipfile\n",
        "\n",
        "        model_url = \"https://alphacephei.com/vosk/models/vosk-model-ru-0.42.zip\"\n",
        "        zip_path = \"vosk-model-ru-0.42.zip\"\n",
        "\n",
        "        urllib.request.urlretrieve(model_url, zip_path)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\".\")\n",
        "        os.remove(zip_path)\n",
        "        print(\"Модель скачана и распакована\")\n",
        "\n",
        "    model = Model(model_path)\n",
        "\n",
        "    wf = wave.open(audio_path, 'rb')\n",
        "\n",
        "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
        "        print(\"Предупреждение: аудио не в идеальном формате, но попробуем обработать...\")\n",
        "\n",
        "    # Создаем распознаватель\n",
        "    rec = KaldiRecognizer(model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    results = []\n",
        "    print(\"Идет распознавание...\")\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        if rec.AcceptWaveform(data):\n",
        "            result = json.loads(rec.Result())\n",
        "            results.append(result)\n",
        "\n",
        "    # Финальный результат\n",
        "    final_result = json.loads(rec.FinalResult())\n",
        "    results.append(final_result)\n",
        "\n",
        "    wf.close()\n",
        "    return results\n",
        "\n",
        "def simple_speaker_segmentation(transcription_results, num_speakers=4):\n",
        "    \"\"\"\n",
        "    Простое разделение на спикеров по паузам\n",
        "    \"\"\"\n",
        "    segments = []\n",
        "    current_speaker = 0\n",
        "    last_end_time = 0\n",
        "\n",
        "    for result in transcription_results:\n",
        "        if 'result' not in result:\n",
        "            continue\n",
        "\n",
        "        for word_info in result['result']:\n",
        "            start = word_info['start']\n",
        "            end = word_info['end']\n",
        "            word = word_info['word']\n",
        "\n",
        "            # Если пауза больше 1.5 секунды - меняем спикера\n",
        "            if start - last_end_time > 1.5:\n",
        "                current_speaker = (current_speaker + 1) % num_speakers\n",
        "\n",
        "            segments.append({\n",
        "                'start': start,\n",
        "                'end': end,\n",
        "                'text': word,\n",
        "                'speaker': f\"SPEAKER_{current_speaker:02d}\"\n",
        "            })\n",
        "            last_end_time = end\n",
        "\n",
        "    return segments\n",
        "\n",
        "def group_segments_by_speaker(segments, time_threshold=2.0):\n",
        "    \"\"\"\n",
        "    Группировка сегментов по спикерам\n",
        "    \"\"\"\n",
        "    grouped = []\n",
        "    current_group = None\n",
        "\n",
        "    for segment in segments:\n",
        "        if current_group is None:\n",
        "            current_group = {\n",
        "                'speaker': segment['speaker'],\n",
        "                'text': segment['text'],\n",
        "                'start': segment['start'],\n",
        "                'end': segment['end']\n",
        "            }\n",
        "        elif (current_group['speaker'] == segment['speaker'] and\n",
        "              segment['start'] - current_group['end'] < time_threshold):\n",
        "            current_group['text'] += \" \" + segment['text']\n",
        "            current_group['end'] = segment['end']\n",
        "        else:\n",
        "            grouped.append(current_group)\n",
        "            current_group = {\n",
        "                'speaker': segment['speaker'],\n",
        "                'text': segment['text'],\n",
        "                'start': segment['start'],\n",
        "                'end': segment['end']\n",
        "            }\n",
        "\n",
        "    if current_group:\n",
        "        grouped.append(current_group)\n",
        "\n",
        "    return grouped\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Форматирование времени\"\"\"\n",
        "    minutes = int(seconds // 60)\n",
        "    seconds = seconds % 60\n",
        "    return f\"{minutes:02d}:{seconds:06.3f}\"\n",
        "\n",
        "def save_results(segments, output_file=\"transcription.txt\"):\n",
        "    \"\"\"Сохранение результатов\"\"\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for segment in segments:\n",
        "            f.write(f\"[{segment['speaker']}] \")\n",
        "            f.write(f\"{format_time(segment['start'])}-{format_time(segment['end'])}\\n\")\n",
        "            f.write(f\"{segment['text']}\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "    print(f\"Результаты сохранены в {output_file}\")\n",
        "\n",
        "# Основная функция\n",
        "def transcribe_mp3_with_speakers(mp3_path, num_speakers=4):\n",
        "    \"\"\"\n",
        "    Транскрибация MP3 файла с разделением на спикеров\n",
        "    \"\"\"\n",
        "    print(\"Начинаем обработку MP3 файла...\")\n",
        "\n",
        "    wav_path = convert_mp3_to_wav(mp3_path)\n",
        "\n",
        "    if not wav_path:\n",
        "        print(\"Ошибка конвертации!\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        results = transcribe_with_vosk(wav_path)\n",
        "        segments = simple_speaker_segmentation(results, num_speakers)\n",
        "\n",
        "        grouped_segments = group_segments_by_speaker(segments)\n",
        "\n",
        "        return grouped_segments\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(wav_path):\n",
        "            os.unlink(wav_path)\n",
        "            print(\"Временный файл удален\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp3_file = \"32140-312ds213-91094-2134.mp3\"\n",
        "\n",
        "    if not os.path.exists(mp3_file):\n",
        "        print(f\"Файл {mp3_file} не найден!\")\n",
        "        print(\"Убедитесь, что файл существует в той же папке\")\n",
        "    else:\n",
        "        # Запуск транскрибации\n",
        "        transcription = transcribe_mp3_with_speakers(mp3_file, num_speakers=4)\n",
        "\n",
        "        if transcription:\n",
        "            # Вывод результатов\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"РЕЗУЛЬТАТЫ ТРАНСКРИБАЦИИ:\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            for segment in transcription:\n",
        "                print(f\"[{segment['speaker']}] {segment['text']}\")\n",
        "                print(f\"Время: {format_time(segment['start'])} - {format_time(segment['end'])}\\n\")\n",
        "\n",
        "            # Сохранение в файл\n",
        "            save_results(transcription, \"transcription_result.txt\")\n",
        "\n",
        "            print(\"Обработка завершена успешно!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
